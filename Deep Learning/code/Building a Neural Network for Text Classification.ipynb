{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network for Text Classification\n",
    "\n",
    "Now that you know how does a Neural Network work and have a basic idea about working with keras, in this notebook you will learn how to implement a NN for Text Classification on a real data set. \n",
    "\n",
    "Let's start without much ado!\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. About the Dataset\n",
    "2. Data Preprocessing for Neural Network\n",
    "     - Label Encoding \n",
    "     - Converting text into sequence of tokens\n",
    "     - Padding \n",
    "3. Building a Neural Network model\n",
    "4. Evaluate the model \n",
    "5. Conclusion\n",
    "\n",
    "<img src=\"https://farm2.staticflickr.com/1834/42271822770_6d2a1d533f_b.jpg\" width=700>\n",
    "\n",
    "### 1. About the Dataset\n",
    "\n",
    "The dataset that you are going to use is a collection of news articles from BBC across 5 major categories, namely:\n",
    " \n",
    " - Business\n",
    " - Entertainment\n",
    " - Politics\n",
    " - Sport\n",
    " - Tech\n",
    "\n",
    "There are a total of 2225 articles in the dataset, which is a mix of all of the above categories. Let's load the dataset using pandas and have a quick look at some of the articles. \n",
    "\n",
    "**Note:** You can get the dataset [here](https://trainings.analyticsvidhya.com/asset-v1:AnalyticsVidhya+LP_DL_2019+2019_T1+type@asset+block@bbc_news_mixed.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cairn shares slump on oil setback\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Egypt to sell off state-owned bank\\n\\nThe Egyp...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cairn shares up on new oil find\\n\\nShares in C...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Low-cost airlines hit Eurotunnel\\n\\nChannel Tu...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Parmalat to return to stockmarket\\n\\nParmalat,...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  Cairn shares slump on oil setback\\n\\nShares in...  business\n",
       "1  Egypt to sell off state-owned bank\\n\\nThe Egyp...  business\n",
       "2  Cairn shares up on new oil find\\n\\nShares in C...  business\n",
       "3  Low-cost airlines hit Eurotunnel\\n\\nChannel Tu...  business\n",
       "4  Parmalat to return to stockmarket\\n\\nParmalat,...  business"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "bbc_news = pd.read_csv('../datasets/bbc_news_mixed.csv')\n",
    "bbc_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing for Neural Network\n",
    "\n",
    "Before you can use text data in a Neural Network, you need to preprocess the data and convert in a format which works best with the NN. Here are the major preprocessing that you will be doing:\n",
    "\n",
    "a. Label encoding the target variable \"label\" and converting it into categorical\n",
    "\n",
    "b. Converting the input text to sequence of tokens\n",
    "\n",
    "c. Padding the sequences to make uniform length\n",
    "\n",
    "Let's start with the first one\n",
    "\n",
    "#### a. Label encoding the target variable \"label\" and converting it into categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# initialize label encoder\n",
    "lencod = LabelEncoder()\n",
    "# encode the text labels into numbers\n",
    "bbc_news.label = lencod.fit_transform(bbc_news.label)\n",
    "# convert labels to categorical form\n",
    "y = to_categorical(bbc_news.label)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Converting the input text to sequence of tokens\n",
    "\n",
    " - Because of the way NN work, you need to convert each text in the form of sequences of tokens. \n",
    " - But before that, let's split the data set into training and test sets. \n",
    " - Then we will tokenize it, label encode the tokens by numbers and represent each text as a sequence of those tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bbc_news['text'], y, test_size=0.2, random_state=42)\n",
    "total_X = X_train.append(X_test)\n",
    "\n",
    "# tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(total_X)\n",
    "\n",
    "# calculate maximum length of sequence and vocab size\n",
    "max_len = total_X.str.split().apply(lambda x: len(x)).max()\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "# convert text as sequence of tokens\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Padding the sequences to make uniform length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1780, 4432)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# pad train and test's text sequences to make them all of uniform length\n",
    "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_len, padding='post')\n",
    "\n",
    "print(X_train_pad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building a Neural Network model\n",
    "\n",
    " - Now that your data is properly preprocessed, it's time to build and train a Neural Network.\n",
    " - You will be using the [Embedding layer](https://keras.io/layers/embeddings/) of keras to create an embedding of 100 dimensions for each sequence.\n",
    " - Let's learn by doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten\n",
    "\n",
    "# embedding size\n",
    "EMBEDDING_SIZE = 100\n",
    "vocab_100 = int(vocab_size/100)\n",
    "\n",
    "# initialize a sequential model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE, input_length=max_len))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(vocab_100, activation='relu'))\n",
    "model.add(Flatten())\n",
    "# add the final layer that will classify into 5 classes\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model is ready, here are some things to note:\n",
    "\n",
    " - You need to compile the model before you can use it, this is done in the last line above where you are also specifying the loss funciton to use, the optimizer and the metric to calculate the performance of the model.\n",
    " - You can read more about [Sequential model](https://keras.io/getting-started/sequential-model-guide/), [Dense](https://keras.io/layers/core/) and [Flatten](https://keras.io/layers/core/) layers.\n",
    " \n",
    "Now that the model is compiled, let's have a look at its summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4432, 100)         3236000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4432, 500)         50500     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4432, 323)         161823    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1431536)           0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 7157685   \n",
      "=================================================================\n",
      "Total params: 10,606,008\n",
      "Trainable params: 10,606,008\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the model\n",
    "\n",
    " - Now that the model is compiled and ready, you can start training the model.\n",
    " - You also need a way to evaluate the model which will be done using the test set that we created earlier.\n",
    " - In keras, you can do all of this in a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1780 samples, validate on 445 samples\n",
      "Epoch 1/11\n",
      "1780/1780 [==============================] - 5s 3ms/step - loss: 1.6410 - acc: 0.3517 - val_loss: 1.1287 - val_acc: 0.4989\n",
      "Epoch 2/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 0.5796 - acc: 0.7730 - val_loss: 0.3111 - val_acc: 0.8787\n",
      "Epoch 3/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 0.0341 - acc: 0.9933 - val_loss: 0.2826 - val_acc: 0.9079\n",
      "Epoch 4/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 0.0045 - acc: 0.9994 - val_loss: 0.2432 - val_acc: 0.9281\n",
      "Epoch 5/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 6.6728e-04 - acc: 1.0000 - val_loss: 0.2289 - val_acc: 0.9281\n",
      "Epoch 6/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 2.0553e-04 - acc: 1.0000 - val_loss: 0.2348 - val_acc: 0.9326\n",
      "Epoch 7/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 1.5452e-04 - acc: 1.0000 - val_loss: 0.2368 - val_acc: 0.9348\n",
      "Epoch 8/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 1.2211e-04 - acc: 1.0000 - val_loss: 0.2361 - val_acc: 0.9326\n",
      "Epoch 9/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 9.9708e-05 - acc: 1.0000 - val_loss: 0.2396 - val_acc: 0.9348\n",
      "Epoch 10/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 8.3283e-05 - acc: 1.0000 - val_loss: 0.2407 - val_acc: 0.9326\n",
      "Epoch 11/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 7.0596e-05 - acc: 1.0000 - val_loss: 0.2404 - val_acc: 0.9348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcd7b4b16d8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and evaluate the model\n",
    "model.fit(X_train_pad, y_train, epochs=11, validation_data=(X_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Conclusion\n",
    "\n",
    " - As you can see above, the \"val_acc\" is the accuracy of the model on the training set. This value gives us an indication as to how well our model is generalizing on unseen data.\n",
    " - With just using a simple network, the accuracy is around 93~94 % which is a really good result.\n",
    " - Notice how easy keras makes it to train a Neural Network by simple stacking layers on top (Sequential).\n",
    " - You can explore this further in [the keras sequential model guide](https://keras.io/getting-started/sequential-model-guide/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
