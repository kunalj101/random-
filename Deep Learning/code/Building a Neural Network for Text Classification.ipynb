{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network for Text Classification\n",
    "\n",
    " - Now that you know how does a Neural Network work and have a basic idea about working with Keras, in this notebook you will learn how to implement a NN for Text Classification on a real data set.\n",
    " - Neural networks are responsible for some of the biggest breakthroughs in AI and related fields of Computer Vision and NLP.\n",
    " - Let's jump right into code and see how to create a NN for text classification!\n",
    " \n",
    "### Table of Contents\n",
    "\n",
    "1. About the Dataset\n",
    "2. Data Preprocessing for Neural Network\n",
    "     - Label Encoding \n",
    "     - Converting text into sequence of tokens\n",
    "     - Padding the sequences\n",
    "3. Building a Neural Network model\n",
    "4. Evaluate the model \n",
    "5. Conclusion\n",
    "\n",
    "![](../images/nn.jpg)\n",
    "\n",
    "### 1. About the Dataset\n",
    "\n",
    "The dataset that you are going to use is a collection of news articles from BBC across 5 major categories, namely:\n",
    " \n",
    " - Business\n",
    " - Entertainment\n",
    " - Politics\n",
    " - Sport\n",
    " - Tech\n",
    "\n",
    "There are a total of 2225 articles in the dataset, which is a mix of all of the above categories. Let's load the dataset using pandas and have a quick look at some of the articles. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cairn shares slump on oil setback\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Egypt to sell off state-owned bank\\n\\nThe Egyp...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cairn shares up on new oil find\\n\\nShares in C...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Low-cost airlines hit Eurotunnel\\n\\nChannel Tu...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Parmalat to return to stockmarket\\n\\nParmalat,...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  Cairn shares slump on oil setback\\n\\nShares in...  business\n",
       "1  Egypt to sell off state-owned bank\\n\\nThe Egyp...  business\n",
       "2  Cairn shares up on new oil find\\n\\nShares in C...  business\n",
       "3  Low-cost airlines hit Eurotunnel\\n\\nChannel Tu...  business\n",
       "4  Parmalat to return to stockmarket\\n\\nParmalat,...  business"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "bbc_news = pd.read_csv('../data/bbc_news_mixed.csv')\n",
    "bbc_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing for Neural Network\n",
    "\n",
    "Before you can use text data in a Neural Network, you need to preprocess the data and convert in a format which works best with the NN. Here are the major preprocessing that you will be doing:\n",
    "\n",
    "1. Label encoding the target variable \"label\" and converting it into categorical\n",
    "\n",
    "2. Converting the input text to sequence of tokens\n",
    "\n",
    "3. Padding the sequences to make uniform length\n",
    "\n",
    "Let's start with the first one\n",
    "\n",
    "#### a. Label encoding the target variable \"label\" and converting it into categorical\n",
    "\n",
    " - We label encode the **label** column from text categories to numbers 0 to 4 to reperesent each of the five categories.\n",
    " - We then convert this into a one hot representation for the neural network. Each row of y would be a list of 5 numbers each reperesenting the classes. If there is 1 at index 2 and rest are 0 this means that example belongs to class 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# initialize label encoder\n",
    "lencod = LabelEncoder()\n",
    "# encode the text labels into numbers\n",
    "bbc_news.label = lencod.fit_transform(bbc_news.label)\n",
    "# convert labels to categorical form\n",
    "y = to_categorical(bbc_news.label)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Converting the input text to sequence of tokens\n",
    "\n",
    " - Because of the way NN work, you need to convert each text in the form of sequences of tokens. \n",
    " - But before that, let's split the data set into training and test sets. \n",
    " - Then we will tokenize it, label encode the tokens by numbers and represent each text as a sequence of those tokens. \n",
    " - Then we calculate the length of the longest sequence and the vocabulary size these two values will be useful when padding the sequences and using the Keras's embedding layer in future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(bbc_news['text'], y, test_size=0.2, random_state=42)\n",
    "total_X = X_train.append(X_test)\n",
    "\n",
    "# tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(total_X)\n",
    "\n",
    "# convert text as sequence of tokens\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# calculate maximum length of sequence and vocab size\n",
    "max_len = total_X.str.split().apply(lambda x: len(x)).max()\n",
    "vocab_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Padding the sequences to make uniform length\n",
    "\n",
    "The sequences have different lengths and Keras prefers inputs to be vectorized and all inputs to have the same length. We will pad all input sequences to have the length of **max_len** because that is the largest sequence size. Again, we can do this with a built in Keras function, in this case the **pad_sequences()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1780, 4432)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# pad train and test's text sequences to make them all of uniform length\n",
    "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_len, padding='post')\n",
    "\n",
    "print(X_train_pad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building a Neural Network model\n",
    "\n",
    " - Now that your data is properly preprocessed, it's time to build and train a Neural Network.\n",
    " - You will be using the [Embedding layer](https://keras.io/layers/embeddings/) of keras to create an embedding of 100 dimensions for each sequence. The embedding layer learns a vector of given length as an embedding for each sequence in your data set. This is in a way similar to the word embeddings that you learned in the previous modules : Word2Vec and GloVe.\n",
    " - Since this layer is available easily in Keras, you can directly use it. You can also create this layer by utilizing Word2Vec or GloVe embeddings of your data set but that might be overkill for some situations.\n",
    " - Let's learn by doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten\n",
    "\n",
    "# embedding size\n",
    "EMBEDDING_SIZE = 100\n",
    "vocab_100 = int(vocab_size/100)\n",
    "\n",
    "# initialize a sequential model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE, input_length=max_len))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(vocab_100, activation='relu'))\n",
    "model.add(Flatten())\n",
    "# add the final layer that will classify into 5 classes\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model is ready, here are some things to note:\n",
    "\n",
    " - You need to compile the model before you can use it, this is done in the last line above where you are also specifying the loss funciton to use, the optimizer and the metric to calculate the performance of the model.\n",
    " - You can read more about [Sequential model](https://keras.io/getting-started/sequential-model-guide/), [Dense](https://keras.io/layers/core/) and [Flatten](https://keras.io/layers/core/) layers.\n",
    " \n",
    "Now that the model is compiled, let's have a look at its summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4432, 100)         3236000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4432, 500)         50500     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4432, 323)         161823    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1431536)           0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 7157685   \n",
      "=================================================================\n",
      "Total params: 10,606,008\n",
      "Trainable params: 10,606,008\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the model\n",
    "\n",
    " - Now that the model is compiled and ready, you can start training the model.\n",
    " - You also need a way to evaluate the model which will be done using the test set that we created earlier.\n",
    " - In keras, you can do all of this in a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1780 samples, validate on 445 samples\n",
      "Epoch 1/11\n",
      "1780/1780 [==============================] - 5s 3ms/step - loss: 1.6953 - acc: 0.2826 - val_loss: 1.5032 - val_acc: 0.2989\n",
      "Epoch 2/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 0.9949 - acc: 0.6152 - val_loss: 0.3647 - val_acc: 0.8629\n",
      "Epoch 3/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 0.0591 - acc: 0.9854 - val_loss: 0.1380 - val_acc: 0.9573\n",
      "Epoch 4/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.1023 - val_acc: 0.9663\n",
      "Epoch 5/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 6.5460e-04 - acc: 1.0000 - val_loss: 0.1035 - val_acc: 0.9708\n",
      "Epoch 6/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 4.2783e-04 - acc: 1.0000 - val_loss: 0.1027 - val_acc: 0.9685\n",
      "Epoch 7/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 3.0492e-04 - acc: 1.0000 - val_loss: 0.1032 - val_acc: 0.9685\n",
      "Epoch 8/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 2.2801e-04 - acc: 1.0000 - val_loss: 0.1025 - val_acc: 0.9685\n",
      "Epoch 9/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 1.7180e-04 - acc: 1.0000 - val_loss: 0.1030 - val_acc: 0.9640\n",
      "Epoch 10/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 1.2429e-04 - acc: 1.0000 - val_loss: 0.1010 - val_acc: 0.9663\n",
      "Epoch 11/11\n",
      "1780/1780 [==============================] - 3s 2ms/step - loss: 8.8172e-05 - acc: 1.0000 - val_loss: 0.1022 - val_acc: 0.9640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efe3cd93710>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and evaluate the model\n",
    "model.fit(X_train_pad, y_train, epochs=11, validation_data=(X_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Conclusion\n",
    "\n",
    " - As you can see above, the \"val_acc\" is the accuracy of the model on the training set. This value gives us an indication as to how well our model is generalizing on unseen data.\n",
    " - With just using a simple network, the accuracy is around 93~97 % which is a really good result.\n",
    " - Notice how easy keras makes it to train a Neural Network by simple stacking layers on top (Sequential).\n",
    " - You can explore this further in [the keras sequential model guide](https://keras.io/getting-started/sequential-model-guide/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
