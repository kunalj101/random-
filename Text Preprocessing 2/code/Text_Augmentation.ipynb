{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data Augmentation\n",
    "\n",
    "Machine Learning (ML) sometimes run into a problem where the training data has limited size and with less data the ML model fails to capture enough variation. To make our model generalize well on new or unseen data we need to have more data. To mitigate this issue we can take help of __Data Augmentation__. \n",
    "\n",
    "![](dandelion.jpg)\n",
    "\n",
    "### What is Data Augmentation?\n",
    "\n",
    "Data Augmentation simply means increasing the number of data points. For example, in computer vision, we generate more images from the already given images with the help of several methods such as image rotation, changing lighting conditions, image cropping etc.\n",
    "\n",
    "### Why augment text data?\n",
    "\n",
    "Majority of the NLP problems are solved by applying a machine learning or deep learning model on the pre-processed text data. Therefore, it makes sense to augment and generate more text data. One solution could be that of collecting more data, but that would come with a cost which can be in terms of money, human effort and off course time consumed in the process. Hence __text data augmentation__ is a better alternatie in such cases.\n",
    "\n",
    "In this notebook we will use the following 3 approaches to create augmented text data:\n",
    "\n",
    "1. Take off words randomly\n",
    "2. Concatenate POS tags to text\n",
    "3. Replace named entities with their categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use spaCy's medium size model in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with the following sample text throughout this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"I visited this place on Valentine's day in the evening. We had to wait for around 15 minutes to get a table here. We utilised that time in getting pictures ;) This place had a very chill vibe with loud music in the background. Live telecast of a cricket match was also going on. It was a little difficult to hold a conversation with someone.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Take off words randomly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach we will try to generate multiple versions of the sample text by omitting a few words randomly from the text. For example, consider this sentence \"Augmenting small datasets is important and challenging\". Its augmented versions would be like...\n",
    "\n",
    "* Augmenting datasets is important challenging\n",
    "* Augmenting small datasets important and \n",
    "* small  is important and challenging\n",
    "\n",
    "So, the idea is pretty simple but quite effective. We will follow the steps given below:\n",
    "\n",
    "1. Tokenize the sample text\n",
    "2. Find the number of tokens\n",
    "3. Set the percentage of words to keep\n",
    "4. Randomly omit the words to generate new text data\n",
    "\n",
    "Sounds interesting? Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sample_text\n",
    "tokens = []\n",
    "for token in doc:\n",
    "    tokens.append(token.text)\n",
    "    \n",
    "# find the number of tokens\n",
    "l = len(tokens)\n",
    "\n",
    "# number of tokens to keep\n",
    "random_tokens_count = round(0.8*l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will genrate 5 versions of augmented text. Feel free to generate as many versions as you like by changing the value of __n__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to store augmented text\n",
    "augmented_texts = []\n",
    "\n",
    "# number of versions of augmented text\n",
    "n = 5\n",
    "\n",
    "for i in range(n):\n",
    "    # randomly generate indices of tokens to keep\n",
    "    new_tokens_index = random.sample(range(l), random_tokens_count)\n",
    "    new_tokens_index.sort()\n",
    "    \n",
    "    # generate augmented list of tokens\n",
    "    new_tokens = [tokens[t] for t in new_tokens_index]\n",
    "    \n",
    "    # create of augmented version of sample_text\n",
    "    augmented_texts.append(' '.join(new_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the text after augmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited this place Valentine 's in evening . We had to wait for around minutes to a table here . We in getting pictures ;) This place had a very chill vibe music in background . Live telecast of a cricket match was also going on . It was a little hold a conversation with .\n",
      "\n",
      "I visited this place 's day in the evening . We wait for around 15 minutes to get a table . We utilised that getting pictures This place had a very chill vibe with loud in the Live telecast of cricket match was also . It was a little difficult to hold a conversation with someone .\n",
      "\n",
      "I visited this place on Valentine day in evening . We had to wait 15 minutes to a here utilised that time getting pictures ;) place had a very chill vibe with loud music in the . Live telecast of a cricket match was going on It was a little difficult to hold a conversation with someone\n",
      "\n",
      "visited place Valentine 's day in the evening . We to wait for around 15 minutes to here . We utilised that time in getting pictures ;) place had a very vibe with loud music in the . Live telecast of cricket match was also going on . It a difficult hold a conversation with someone .\n",
      "\n",
      "I this on Valentine 's day in the evening . We had to for around 15 minutes to get a table here . We utilised time in getting pictures ;) This place had a vibe with music in background . Live telecast of a cricket match was going on . It a little conversation with someone .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print augmented texts\n",
    "for i in augmented_texts:\n",
    "    print(i + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Concatenate POS Tags to Text\n",
    "\n",
    "As we have already seen in the introductory video of Part of Speech (POS) Tagging, POS tags define the role of words in a sentence based on the context. Let's look at the example below: \n",
    "\n",
    "* She writes radio and television <span style=\"color:green\">__plays__</span>.\n",
    "* He <span style=\"color:blue\">__plays__</span> football in the park.\n",
    "\n",
    "The word \"plays\" in the first sentence is a noun and in the second sentence is a verb. However, our system would treat it as the same. Therefore, POS tagging such words comes handy. We will concatenate POS tags to the words in the text and augment it with the help of spaCy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sample_text\n",
    "tokens_pos = []\n",
    "for token in doc:\n",
    "    tokens_pos.append(token.text+\"_\"+token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I_PRON', 'visited_VERB', 'this_DET', 'place_NOUN', 'on_ADP', 'Valentine_PROPN', \"'s_PART\", 'day_NOUN', 'in_ADP', 'the_DET', 'evening_NOUN', '._PUNCT', 'We_PRON', 'had_VERB', 'to_PART', 'wait_VERB', 'for_ADP', 'around_ADV', '15_NUM', 'minutes_NOUN', 'to_PART', 'get_VERB', 'a_DET', 'table_NOUN', 'here_ADV', '._PUNCT', 'We_PRON', 'utilised_VERB', 'that_DET', 'time_NOUN', 'in_ADP', 'getting_VERB', 'pictures_NOUN', ';)_PUNCT', 'This_DET', 'place_NOUN', 'had_VERB', 'a_DET', 'very_ADV', 'chill_ADJ', 'vibe_NOUN', 'with_ADP', 'loud_ADJ', 'music_NOUN', 'in_ADP', 'the_DET', 'background_NOUN', '._PUNCT', 'Live_ADJ', 'telecast_NOUN', 'of_ADP', 'a_DET', 'cricket_NOUN', 'match_NOUN', 'was_VERB', 'also_ADV', 'going_VERB', 'on_PART', '._PUNCT', 'It_PRON', 'was_VERB', 'a_DET', 'little_ADJ', 'difficult_ADJ', 'to_PART', 'hold_VERB', 'a_DET', 'conversation_NOUN', 'with_ADP', 'someone_NOUN', '._PUNCT']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_PRON visited_VERB this_DET place_NOUN on_ADP Valentine_PROPN 's_PART day_NOUN in_ADP the_DET evening_NOUN ._PUNCT We_PRON had_VERB to_PART wait_VERB for_ADP around_ADV 15_NUM minutes_NOUN to_PART get_VERB a_DET table_NOUN here_ADV ._PUNCT We_PRON utilised_VERB that_DET time_NOUN in_ADP getting_VERB pictures_NOUN ;)_PUNCT This_DET place_NOUN had_VERB a_DET very_ADV chill_ADJ vibe_NOUN with_ADP loud_ADJ music_NOUN in_ADP the_DET background_NOUN ._PUNCT Live_ADJ telecast_NOUN of_ADP a_DET cricket_NOUN match_NOUN was_VERB also_ADV going_VERB on_PART ._PUNCT It_PRON was_VERB a_DET little_ADJ difficult_ADJ to_PART hold_VERB a_DET conversation_NOUN with_ADP someone_NOUN ._PUNCT\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(tokens_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Replace Named Entities with their Categories\n",
    "\n",
    "Now we will see how we can use _named entity recognition_ to augment our text data. In this technique, we will replace named entities with their respective broader categories. \n",
    "\n",
    "For example, in the sentence <span style=\"color:green\">\"Apple is looking at buying a U.K. startup.\"</span>, the entity __Apple__ is an organization and __U.K.__ is a geopolitical entity. We will replace both the entities with the tags ORG and GPE, respectively. Hence, the augmented sentence would be...\n",
    "\n",
    "<span style=\"color:magenta\">\"__ORG__ is looking at buying a __GPE__ startup.\"</span>\n",
    "\n",
    "This type of augment is well suited for those tasks where we are not too bothered about the name of the entities. Let try it out on the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited this place on DATE in TIME. We had to wait for TIME to get a table here. We utilised that time in getting pictures ;) This place had a very chill vibe with loud music in the background. Live telecast of a cricket match was also going on. It was a little difficult to hold a conversation with someone.\n"
     ]
    }
   ],
   "source": [
    "aug_text = sample_text\n",
    "for ent in doc.ents:\n",
    "    aug_text = re.sub(ent.text, ent.label_, aug_text)\n",
    "    \n",
    "print(aug_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to https://www.yelp.com/dataset and download the Yelp dataset and try the above mentioned techniques to augment the data.\n",
    "\n",
    "![](yelp_fullcolor.png)\n",
    "\n",
    "If you have any other text data augmentation techniques, then feel free to share it in the discussion forum. Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
