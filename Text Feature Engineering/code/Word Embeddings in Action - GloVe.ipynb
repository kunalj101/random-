{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings in Action - GloVe\n",
    "\n",
    " - [Stanford NLP Group](https://nlp.stanford.edu/) have released their own algoritm for training word embedding like Word2Vec and it's called [Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/) or GloVe for short.\n",
    " - Researchers at the moment prefer GloVe over Word2Vec based on results and in this notebook, you will learn how to utilize it on a text classification problem. \n",
    " \n",
    "With that expectation set, let's start without much ado!\n",
    "\n",
    "![](../images/glove.jpg)\n",
    "\n",
    "### Table of Contents\n",
    " \n",
    "1. About the dataset\n",
    "2. Comparing GloVe vs Word2Vec\n",
    "3. Utilizing Stanford NLP's pretrained GloVe\n",
    "    - Installing\n",
    "    - Saving embedding matrix to disk\n",
    "    - Finding most similar words given a context word\n",
    "    - Contextual relationship between words \n",
    "4. Challenge : Build a text classification model using GloVe\n",
    "\n",
    "### 1. About the dataset\n",
    "\n",
    "The dataset that you are going to use is a collection of news articles from BBC across 5 major categories, namely:\n",
    " \n",
    " - Business\n",
    " - Entertainment\n",
    " - Politics\n",
    " - Sport\n",
    " - Tech\n",
    "\n",
    "There are a total of 2225 articles in the dataset, which is a mix of all of the above categories. Let's load the dataset using pandas and have a quick look at some of the articles. \n",
    "\n",
    "**Note:** You can get the dataset [here.](https://trainings.analyticsvidhya.com/asset-v1:AnalyticsVidhya+LP_DL_2019+2019_T1+type@asset+block@bbc_news_mixed.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cairn shares slump on oil setback\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Egypt to sell off state-owned bank\\n\\nThe Egyp...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cairn shares up on new oil find\\n\\nShares in C...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Low-cost airlines hit Eurotunnel\\n\\nChannel Tu...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Parmalat to return to stockmarket\\n\\nParmalat,...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  Cairn shares slump on oil setback\\n\\nShares in...  business\n",
       "1  Egypt to sell off state-owned bank\\n\\nThe Egyp...  business\n",
       "2  Cairn shares up on new oil find\\n\\nShares in C...  business\n",
       "3  Low-cost airlines hit Eurotunnel\\n\\nChannel Tu...  business\n",
       "4  Parmalat to return to stockmarket\\n\\nParmalat,...  business"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "bbc_news = pd.read_csv('../datasets/bbc_news_mixed.csv')\n",
    "bbc_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 2 articles\n",
    "for art in bbc_news.text[:2]:\n",
    "    print(art[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have an idea of how your data looks like, let's see the count of each category in the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# category-wise count\n",
    "bbc_news.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Comparing GloVe vs Word2Vec\n",
    "\n",
    "The following are major differences between GloVe and Word2Vec models - \n",
    "\n",
    "- GloVe works similarly as Word2Vec. While Word2Vec is a **predictive model** that predicts context given a word (or vice-versa based on if you are using skip-gram or cbow variant),\n",
    "\n",
    "- GloVe learns by constructing a **co-occurrence matrix** (words x context) that basically counts how frequently a word appears in a context. Since it's going to be a gigantic matrix, we factorize this matrix to achieve a lower-dimension representation. There's a lot of details that goes in GloVe but that's the rough idea. \n",
    " \n",
    "- In word2vec, skip-gram models try to capture co-occurrence **one window at a time**.\n",
    "\n",
    "- In Glove it tries to capture the counts of **overall statistics** how often it appears.\n",
    "\n",
    "In practice, the main difference is that GloVe embeddings work better on some data sets, while word2vec embeddings work better on others. They both do very well at capturing the semantics of analogy, and that takes us, it turns out, a very long way toward lexical semantics in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Utilizing Stanford NLP's pretrained GloVe\n",
    "\n",
    "It's time to quickly start using GloVe in your problem statement, but before you can do that you need to install GloVe pre-trained embeddings. \n",
    "\n",
    "#### a. Installing\n",
    "\n",
    " - Download the GloVe model from [Glove6B.zip](http://nlp.stanford.edu/data/glove.6B.zip). This is the smallest model available which is trained on Wikipedia's text containing 6 Billion tokens and a vocabulary of around 400,000 words. The file is **822 MB** in size so it will take some time to download.\n",
    " - Extract the zip file by the following command\n",
    " \n",
    " `unzip glove.6B.zip`\n",
    " - Once you have extracted the file, you will see that there are multiple text files\n",
    "     1. **glove.6B.100d.txt** - Contains 100 dimension vectors for each word of the vocabulary.\n",
    "     2. **glove.6B.200d.txt** - Contains 200 dimension vectors for each word of the vocabulary.\n",
    "     3. **glove.6B.300d.txt** - Contains 300 dimension vectors for each word of the vocabulary.\n",
    "     4. **glove.6B.50d.txt**  - Contains 50 dimension vectors for each word of the vocabulary.\n",
    "     \n",
    " - Basicially based on your requirement, you can choose the dimensions of your vectors. In this case, you will work with the 300 dimensional one. \n",
    " - Let's quickly load the vectors in python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.046560</td>\n",
       "      <td>0.213180</td>\n",
       "      <td>-0.007436</td>\n",
       "      <td>-0.458540</td>\n",
       "      <td>-0.035639</td>\n",
       "      <td>0.236430</td>\n",
       "      <td>-0.288360</td>\n",
       "      <td>0.215210</td>\n",
       "      <td>-0.134860</td>\n",
       "      <td>-1.6413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013064</td>\n",
       "      <td>-0.296860</td>\n",
       "      <td>-0.079913</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.031549</td>\n",
       "      <td>0.285060</td>\n",
       "      <td>-0.087461</td>\n",
       "      <td>0.009061</td>\n",
       "      <td>-0.209890</td>\n",
       "      <td>0.053913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.255390</td>\n",
       "      <td>-0.257230</td>\n",
       "      <td>0.131690</td>\n",
       "      <td>-0.042688</td>\n",
       "      <td>0.218170</td>\n",
       "      <td>-0.022702</td>\n",
       "      <td>-0.178540</td>\n",
       "      <td>0.107560</td>\n",
       "      <td>0.058936</td>\n",
       "      <td>-1.3854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075968</td>\n",
       "      <td>-0.014359</td>\n",
       "      <td>-0.073794</td>\n",
       "      <td>0.221760</td>\n",
       "      <td>0.146520</td>\n",
       "      <td>0.566860</td>\n",
       "      <td>0.053307</td>\n",
       "      <td>-0.232900</td>\n",
       "      <td>-0.122260</td>\n",
       "      <td>0.354990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>-0.125590</td>\n",
       "      <td>0.013630</td>\n",
       "      <td>0.103060</td>\n",
       "      <td>-0.101230</td>\n",
       "      <td>0.098128</td>\n",
       "      <td>0.136270</td>\n",
       "      <td>-0.107210</td>\n",
       "      <td>0.236970</td>\n",
       "      <td>0.328700</td>\n",
       "      <td>-1.6785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060148</td>\n",
       "      <td>-0.156190</td>\n",
       "      <td>-0.119490</td>\n",
       "      <td>0.234450</td>\n",
       "      <td>0.081367</td>\n",
       "      <td>0.246180</td>\n",
       "      <td>-0.152420</td>\n",
       "      <td>-0.342240</td>\n",
       "      <td>-0.022394</td>\n",
       "      <td>0.136840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-0.076947</td>\n",
       "      <td>-0.021211</td>\n",
       "      <td>0.212710</td>\n",
       "      <td>-0.722320</td>\n",
       "      <td>-0.139880</td>\n",
       "      <td>-0.122340</td>\n",
       "      <td>-0.175210</td>\n",
       "      <td>0.121370</td>\n",
       "      <td>-0.070866</td>\n",
       "      <td>-1.5721</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.366730</td>\n",
       "      <td>-0.386030</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.015747</td>\n",
       "      <td>0.340360</td>\n",
       "      <td>0.478410</td>\n",
       "      <td>0.068617</td>\n",
       "      <td>0.183510</td>\n",
       "      <td>-0.291830</td>\n",
       "      <td>-0.046533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-0.257560</td>\n",
       "      <td>-0.057132</td>\n",
       "      <td>-0.671900</td>\n",
       "      <td>-0.380820</td>\n",
       "      <td>-0.364210</td>\n",
       "      <td>-0.082155</td>\n",
       "      <td>-0.010955</td>\n",
       "      <td>-0.082047</td>\n",
       "      <td>0.460560</td>\n",
       "      <td>-1.8477</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012806</td>\n",
       "      <td>-0.597070</td>\n",
       "      <td>0.317340</td>\n",
       "      <td>-0.252670</td>\n",
       "      <td>0.543840</td>\n",
       "      <td>0.063007</td>\n",
       "      <td>-0.049795</td>\n",
       "      <td>-0.160430</td>\n",
       "      <td>0.046744</td>\n",
       "      <td>-0.070621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7    \\\n",
       "0                                                                           \n",
       "the  0.046560  0.213180 -0.007436 -0.458540 -0.035639  0.236430 -0.288360   \n",
       ",   -0.255390 -0.257230  0.131690 -0.042688  0.218170 -0.022702 -0.178540   \n",
       ".   -0.125590  0.013630  0.103060 -0.101230  0.098128  0.136270 -0.107210   \n",
       "of  -0.076947 -0.021211  0.212710 -0.722320 -0.139880 -0.122340 -0.175210   \n",
       "to  -0.257560 -0.057132 -0.671900 -0.380820 -0.364210 -0.082155 -0.010955   \n",
       "\n",
       "          8         9       10     ...          291       292       293  \\\n",
       "0                                  ...                                    \n",
       "the  0.215210 -0.134860 -1.6413    ...    -0.013064 -0.296860 -0.079913   \n",
       ",    0.107560  0.058936 -1.3854    ...     0.075968 -0.014359 -0.073794   \n",
       ".    0.236970  0.328700 -1.6785    ...     0.060148 -0.156190 -0.119490   \n",
       "of   0.121370 -0.070866 -1.5721    ...    -0.366730 -0.386030  0.302900   \n",
       "to  -0.082047  0.460560 -1.8477    ...    -0.012806 -0.597070  0.317340   \n",
       "\n",
       "          294       295       296       297       298       299       300  \n",
       "0                                                                          \n",
       "the  0.195000  0.031549  0.285060 -0.087461  0.009061 -0.209890  0.053913  \n",
       ",    0.221760  0.146520  0.566860  0.053307 -0.232900 -0.122260  0.354990  \n",
       ".    0.234450  0.081367  0.246180 -0.152420 -0.342240 -0.022394  0.136840  \n",
       "of   0.015747  0.340360  0.478410  0.068617  0.183510 -0.291830 -0.046533  \n",
       "to  -0.252670  0.543840  0.063007 -0.049795 -0.160430  0.046744 -0.070621  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load glove into a data frame \n",
    "df = pd.read_csv('../embeddings/glove.6B.300d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "# show first 5 columns of the data frame \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - As you can see above, each word is reperesented by 300 real number values. These 300 values together form the glove vector for that particular word. \n",
    "\n",
    " - Before you can use GloVe, it is advisable to convert it into a dictionary like structure. Where each word will be the key of the dictionary and the value would be the 300 dimensional vector. The following code does that, note that based on your system's RAM it might take some time for it to load all the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of vector representation of 'cake': 300\n"
     ]
    }
   ],
   "source": [
    "# make a dictionary of glove\n",
    "glove = {key: val.values for key, val in df.T.items()}\n",
    "# print shape of a vector\n",
    "print('Shape of vector representation of \\'cake\\':', len(glove['cake']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Saving embedding matrix to disk\n",
    "\n",
    "- You'd have noticed that it takes quite sometime to load the glove vectors and convert them to the dictionary format.    \n",
    "\n",
    "- Therefore, there is an easier way to save this dictionary to your computer and load it in a shorter time whenever you want.\n",
    "- We will use python's [pickle](https://docs.python.org/3/library/pickle.html) model to do that. Pickle is a file format that let's you save python objects directly to disk and re-load them in python in the same format they were initially.\n",
    "- Pickle is also used in industry to save a trained model and deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('glove.6B.300d.pkl', 'wb') as fp:\n",
    "    # save the pickle file to disk\n",
    "    pickle.dump(glove, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Finding most similar words given a context word\n",
    "\n",
    " - Currently, we have created a dictionary of words and their glove vectors. Something like this,\n",
    "\n",
    "![](../images/glove_dict.png)\n",
    " - It would be better if we add an index to each word in our glove dictionary. This will help us identify each word by it's index when finding most similar words. The following code returns an `inverse_dictionary` that contains index as key and word as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 entries in the dictionary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 'the'),\n",
       " (1, ','),\n",
       " (2, '.'),\n",
       " (3, 'of'),\n",
       " (4, 'to'),\n",
       " (5, 'and'),\n",
       " (6, 'in'),\n",
       " (7, 'a'),\n",
       " (8, '\"'),\n",
       " (9, \"'s\")]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_inverse_dictionary():\n",
    "    # assign ids to each word\n",
    "    ids = [x for x in range(len(glove))]\n",
    "    # generate inverse dictionary\n",
    "    inverse_dictionary = {v:k for k, v in zip(glove.keys(), ids)}\n",
    "    \n",
    "    return inverse_dictionary\n",
    "\n",
    "# call inverse_dictionary\n",
    "print('First 10 entries in the dictionary:') \n",
    "list(generate_inverse_dictionary().items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Similarity score between two words is calculated by taking [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) of their vectors. You don't need to worry much about what is cosine similarity right now, but do know that this is the same method that **gensim** uses under the hood.\n",
    " - Going by that logic, if you want to find all the similar words to a given word then you will have to find cosine similarity of this word with all the 400,000 words present in the corpus. \n",
    " - This approach might work but will take too long to compute. A better way would be to use matrix multiplication to compute all the similarities and then sort the word indexes (that we created earlier in inverse_dictionary) in descending order, based on their similarity score with the given word.\n",
    " - You can then use the same inverse_dictionary to look up the words using their indexes.\n",
    "![](../images/most_sim.png)\n",
    " - In the above example, we have taken four words 'hi', 'hello', 'no' and 'hey' and assumed their vector to be of length 3 each.\n",
    " - As you can see, matrix multiplication gave the similarity score of 'hey' with each word in the corpus. Utilizing this, we took the index of the word and got the word name from the `inverse_dictionary`.\n",
    " - The following code does that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most similar words; given a context word\n",
    "def most_similar(word_vec, topn=5):\n",
    "    # fetch inverse dictionary\n",
    "    inverse_dictionary = generate_inverse_dictionary()\n",
    "    # fetch glove vectors\n",
    "    word_vectors = list(glove.values())\n",
    "    \n",
    "    # compute cosine_similarity\n",
    "    cosine_similarity = (np.dot(word_vectors, word_vec)\n",
    "           / np.linalg.norm(word_vectors, axis=1)\n",
    "           / np.linalg.norm(word_vec))\n",
    "    \n",
    "    # sort the word ids in descending order based on their similarity score\n",
    "    word_ids = np.argsort(-cosine_similarity)\n",
    "    \n",
    "    # return the most similar words with their similarity score\n",
    "    return [(inverse_dictionary[x], cosine_similarity[x]) for x in word_ids[1:topn]\n",
    "if x in inverse_dictionary]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6336468701479963),\n",
       " ('prince', 0.6196623000643996),\n",
       " ('monarch', 0.5899620887183682),\n",
       " ('kingdom', 0.5791266501891081)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find most similar words to king\n",
    "most_similar(glove['king'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Contextual relationship between words\n",
    "\n",
    " - One of the impressive things about GloVe is it's ability to capture semantic relationship between words. That is the reason that you can do cool stuff like perform linear algebra on words and get an appropriate output. Have a look at the following example:\n",
    "\n",
    "    `airplane - fly + drive = car`\n",
    "\n",
    " - If you pass the left hand side of the above equation to the model, it will give the right handside. Which makes sense because what would you get if you remove the ability to fly from an airplane? And add the ability to drive? You would get a car!\n",
    " - The underlying model is able to understand implicit relationship between airplane and fly and also how removing the medium of travel changes the machine used to travel. \n",
    " - It is also able to understand **how what fly is to airplane similarly drive is to a car.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('car', 0.572202522968639),\n",
       " ('drives', 0.538060840892117),\n",
       " ('vehicle', 0.5147907611059586),\n",
       " ('truck', 0.4797217336287158)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find airplane - fly + drive\n",
    "result = glove['airplane'] - glove['fly'] + glove['drive']\n",
    "# find most similar to result\n",
    "most_similar(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Challenge : Build a text classification model using GloVe\n",
    "\n",
    "Now it is time for you to apply all the learnings so far and build a text classification model on the same BBC News Data set that we used in the Word2Vec notebook. Here are steps that you need to follow to do the same - \n",
    "\n",
    "1. Load glove from the disk.\n",
    "2. Create X by generating glove vectors for each news article.\n",
    "3. Label encode y by using LabelEncoder from sklearn.\n",
    "4. Split the data into 'train' and 'test' sets.\n",
    "5. Train a naive bayes model.\n",
    "6. Compute the accuracy of the model to measure performance.\n",
    "\n",
    "For your convenience, 1 and 2 is already done for you. You may proceed with 3 to 6.\n",
    "\n",
    "**Note: Feel free to refer the previous Word2Vec notebook for ideas/inspiration/code for this assignment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading GloVe vectors from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('glove.6B.300d.pkl', 'rb') as fp:\n",
    "    # load glove from disk \n",
    "    glove = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create X by generating glove vectors for each news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns vector reperesentation of a given word if it is present in vocabulary\n",
    "def get_embedding_glove(doc_tokens):\n",
    "    embeddings = []\n",
    "    # iterate over tokens to extract their vectors    \n",
    "    for tok in doc_tokens:\n",
    "        if tok in glove:\n",
    "            embeddings.append(glove[tok])\n",
    "    # mean the vectors of individual words to get the vector of the statement\n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2225, 300)\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# preprocess all the articles of the data set\n",
    "preprocessed_bbc = bbc_news.text.apply(lambda x: simple_preprocess(x))\n",
    "\n",
    "# create X from glove\n",
    "X = preprocessed_bbc.apply(lambda x: get_embedding_glove(x))\n",
    "X = pd.DataFrame(X.tolist())\n",
    "print('X shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
