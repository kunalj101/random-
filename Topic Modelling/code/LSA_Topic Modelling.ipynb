{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling using Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the 20 Newsgroup dataset from sklearn for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "# check number of documents\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check document categories\n",
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 11,314 text documents distributed across 20 different newsgroups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "To start with, we will try to clean our text data as much as possible. We will remove the punctuations, numbers, special characters all in one step using the regex replace(\"[^a-zA-Z#]\", \" \") which will replace everything, except alphabets, with a space. Then we will remove shorter words because usually they don't contain much of the information. Finally we will make all the text lowercase to nullify case sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "\n",
    "# removing everything except alphabets`\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# removing short words\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# make all text lowercase\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good practice to remove the stop-words from the text data as they are mostly clutter and hardly add carry any information. Stop-words are terms like 'it', 'they', 'am', 'been', 'about', 'because', 'while', etc. To remove the stop-words from the documents text we will have to tokenize the text, i.e., splitting the string of text into individual tokens or words. We will stitch the tokens back together once we have removed the stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# tokenization\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) \n",
    "\n",
    "# remove stop-words\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# de-tokenization\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "    \n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-Term Matrix\n",
    "\n",
    "This is the first step towards topic modeling. We will use sklearn's TfidfVectorizer to create a document-term matrix with 1000 terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             max_features= 1000, # keep top 1000 terms \n",
    "                             max_df = 0.5, \n",
    "                             smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "\n",
    "X.shape # check shape of the document-term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have used all the terms to create this matrix but that would need quite a lot of computation. Hence, we have restricted the number of features to 1000.\n",
    "\n",
    "## Topic Modeling\n",
    "\n",
    "The next step is to represent each and every term and document as a vector. We will use the document-term matrix and decompose it into multiple matrices. We will use sklearn's TruncatedSVD to perform the task of matrix decomposition. Since the data comes from 20 different newsgroups, let's try to have 20 topics for our text data. The number of topics can be specified by using the n_components parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# SVD represent documents and terms in vectors \n",
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "svd_model.fit(X)\n",
    "\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components of svd_model are our topics and we can access them using svdmodel.components. Finally let's print a few most important words in each of the 20 topics and see how our model has done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: \n",
      "['like', 'know', 'people', 'think', 'good', 'time', 'thanks', 'make', 'right', 'want', 'need', 'really', 'problem', 'used', 'work', 'windows', 'year', 'said', 'believe', 'going']\n",
      "\n",
      "\n",
      "Topic 2: \n",
      "['thanks', 'windows', 'card', 'drive', 'mail', 'file', 'advance', 'files', 'software', 'program', 'help', 'video', 'email', 'looking', 'disk', 'using', 'info', 'problem', 'window', 'know']\n",
      "\n",
      "\n",
      "Topic 3: \n",
      "['game', 'team', 'year', 'games', 'season', 'players', 'good', 'play', 'hockey', 'league', 'player', 'drive', 'best', 'teams', 'baseball', 'better', 'runs', 'sale', 'like', 'played']\n",
      "\n",
      "\n",
      "Topic 4: \n",
      "['drive', 'scsi', 'disk', 'hard', 'card', 'drives', 'problem', 'controller', 'floppy', 'power', 'apple', 'speed', 'work', 'computer', 'video', 'problems', 'monitor', 'used', 'internal', 'cable']\n",
      "\n",
      "\n",
      "Topic 5: \n",
      "['windows', 'file', 'window', 'files', 'program', 'using', 'problem', 'running', 'version', 'team', 'screen', 'game', 'application', 'server', 'think', 'directory', 'display', 'drivers', 'manager', 'year']\n",
      "\n",
      "\n",
      "Topic 6: \n",
      "['government', 'chip', 'mail', 'space', 'information', 'encryption', 'data', 'sale', 'email', 'clipper', 'phone', 'send', 'address', 'number', 'used', 'public', 'interested', 'available', 'keys', 'list']\n",
      "\n",
      "\n",
      "Topic 7: \n",
      "['like', 'bike', 'know', 'chip', 'sounds', 'looks', 'look', 'sure', 'clipper', 'going', 'encryption', 'really', 'good', 'keys', 'thing', 'make', 'sound', 'need', 'speed', 'want']\n",
      "\n",
      "\n",
      "Topic 8: \n",
      "['card', 'sale', 'video', 'offer', 'monitor', 'price', 'jesus', 'condition', 'good', 'shipping', 'drivers', 'cards', 'best', 'sell', 'asking', 'people', 'mail', 'color', 'email', 'interested']\n",
      "\n",
      "\n",
      "Topic 9: \n",
      "['know', 'card', 'chip', 'video', 'government', 'people', 'clipper', 'drivers', 'encryption', 'cards', 'right', 'driver', 'monitor', 'keys', 'going', 'serial', 'game', 'need', 'anybody', 'chips']\n",
      "\n",
      "\n",
      "Topic 10: \n",
      "['good', 'know', 'time', 'bike', 'jesus', 'problem', 'work', 'want', 'long', 'bible', 'really', 'used', 'faith', 'engine', 'thing', 'course', 'book', 'question', 'better', 'little']\n",
      "\n",
      "\n",
      "Topic 11: \n",
      "['think', 'chip', 'good', 'thanks', 'clipper', 'need', 'encryption', 'mail', 'people', 'address', 'keys', 'really', 'government', 'data', 'phone', 'space', 'better', 'algorithm', 'escrow', 'pretty']\n",
      "\n",
      "\n",
      "Topic 12: \n",
      "['thanks', 'right', 'problem', 'good', 'bike', 'time', 'window', 'people', 'advance', 'israel', 'left', 'armenian', 'year', 'government', 'windows', 'work', 'armenians', 'israeli', 'engine', 'going']\n",
      "\n",
      "\n",
      "Topic 13: \n",
      "['good', 'people', 'windows', 'know', 'file', 'sale', 'files', 'price', 'condition', 'offer', 'year', 'bike', 'shipping', 'drive', 'list', 'asking', 'interested', 'excellent', 'best', 'sell']\n",
      "\n",
      "\n",
      "Topic 14: \n",
      "['space', 'think', 'know', 'nasa', 'problem', 'year', 'israel', 'time', 'years', 'article', 'shuttle', 'launch', 'monitor', 'armenian', 'sale', 'said', 'orbit', 'news', 'program', 'station']\n",
      "\n",
      "\n",
      "Topic 15: \n",
      "['space', 'good', 'card', 'people', 'time', 'nasa', 'thanks', 'year', 'like', 'video', 'data', 'shuttle', 'heard', 'launch', 'world', 'drivers', 'scsi', 'orbit', 'things', 'team']\n",
      "\n",
      "\n",
      "Topic 16: \n",
      "['people', 'problem', 'window', 'time', 'game', 'want', 'bike', 'using', 'work', 'send', 'server', 'email', 'screen', 'motif', 'need', 'jesus', 'list', 'sale', 'running', 'apple']\n",
      "\n",
      "\n",
      "Topic 17: \n",
      "['time', 'bike', 'right', 'windows', 'file', 'need', 'really', 'card', 'files', 'going', 'jesus', 'space', 'left', 'mail', 'sure', 'speed', 'looking', 'long', 'game', 'drivers']\n",
      "\n",
      "\n",
      "Topic 18: \n",
      "['time', 'problem', 'file', 'think', 'israel', 'long', 'mail', 'armenian', 'said', 'email', 'address', 'armenians', 'post', 'sale', 'game', 'know', 'chip', 'israeli', 'list', 'turkish']\n",
      "\n",
      "\n",
      "Topic 19: \n",
      "['file', 'need', 'card', 'files', 'problem', 'right', 'good', 'game', 'help', 'want', 'format', 'looking', 'people', 'video', 'point', 'mail', 'graphics', 'make', 'read', 'directory']\n",
      "\n",
      "\n",
      "Topic 20: \n",
      "['problem', 'file', 'thanks', 'used', 'space', 'chip', 'sale', 'files', 'game', 'jesus', 'bike', 'offer', 'work', 'said', 'power', 'condition', 'clipper', 'make', 'shipping', 'sound']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:20]\n",
    "    print(\"Topic \"+str(i+1)+\": \")\n",
    "    topics = []\n",
    "    for t in sorted_terms:\n",
    "        topics.append(t[0])\n",
    "    print(topics)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
